{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2b5996",
   "metadata": {
    "id": "MaqQyaO64c4p"
   },
   "source": [
    "In this tutorial, we will cover how to import and run an environment for RL training using OpenAI Gym. We will also look at how well an RL agent trained on this environment to do a specific task performs, compared to an agent taking random actions in the environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db0402",
   "metadata": {
    "id": "qUmJWFY9qypV"
   },
   "outputs": [],
   "source": [
    "#Installing packages for rendering the game on Colab\n",
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!apt-get update > /dev/null 2>&1\n",
    "!apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1\n",
    "!pip install gym[atari] > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3bb454b",
   "metadata": {
    "id": "zu1cshEw7hKu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc296c",
   "metadata": {
    "id": "wgxkZn9t5OaU"
   },
   "source": [
    "# **Setting up the environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaafb87",
   "metadata": {
    "id": "HlD6Nivn6I4c"
   },
   "source": [
    "Any new RL algorithm that is developed will need to be benchmarked and compared against state-of-the-art RL algorithms. A variety of benchmarks exist for various types of tasks and various specific domains eg. robot locomotion. \n",
    "\n",
    "While the final aim is to run such algorithms in the real world eg. controlling a quadruped robot to jump over obstacles, for practicality and reduced costs during development, it is more convenient to use benchmarks that allow virtual simulations of these real-world tasks. A few examples of these are MuJoCo (Multi-Joint dynamics with Contact), StarCraft2 and Atari.\n",
    "\n",
    "OpenAI Gym aims to provide an easy-to-setup general-intelligence benchmark with a wide variety of different environments. The goal is to standardize how environments are defined in AI research publications so that published research becomes more easily reproducible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ad62d",
   "metadata": {
    "id": "_DtMh6tl5Yni"
   },
   "source": [
    "**Import OpenAI gym**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5d5e47",
   "metadata": {
    "id": "213553cd"
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fe8d3",
   "metadata": {
    "id": "kryDSmG77kk4"
   },
   "source": [
    "![rl.png](data:image/png;base64,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)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf2842",
   "metadata": {
    "id": "mwpIjXIZAOhL"
   },
   "source": [
    "Consider an RL agent that interacts with its environment in discrete timesteps. For every discrete time step, the agent perceives the state **s<sub>t</sub>** of its environment and chooses an action **a<sub>t</sub>** according to its **policy**. The agent then receives a reward **r<sub>t+1</sub>** for its action and the environment transitioned into the next state **s<sub>t+1</sub>**. We now show how to set up such an environment using OpenAI gym\n",
    "\n",
    "Some helpful terminology:\n",
    "* **Episode** - A collection of steps that terminates when the agent fails to achieve the goal, or the episode reaches the maximum number of allowed steps.\n",
    "* **Render** - Gym can render one frame for display after each episode.\n",
    "* **Nondeterministic** - For some environments, randomness is a factor in deciding what effects actions have on reward and changes to the observation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b823923",
   "metadata": {
    "id": "QLtauTHkBhYb"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7eca3",
   "metadata": {
    "id": "-o4AVRvGBiod"
   },
   "source": [
    "We just created an instance of the CartPole-v1 environment. \n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "\n",
    "Now let's take a deeper look at some of the environment specifics:\n",
    "* **action space**: What actions can we take on the environment, at each step/episode, to alter the environment.\n",
    "* **observation space**: What is the current state of the portion of the environment that we can observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "598aa128",
   "metadata": {
    "id": "_u_3pJGqCb9e"
   },
   "outputs": [],
   "source": [
    "def query_environment(env, name):\n",
    "  spec = gym.spec(name)\n",
    "  print(f\"Action Space: {env.action_space}\")\n",
    "  print(f\"Observation Space: {env.observation_space}\")\n",
    "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
    "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
    "  print(f\"Reward Range: {env.reward_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2963d",
   "metadata": {
    "id": "RmT3G-SIChK3"
   },
   "outputs": [],
   "source": [
    "query_environment(env, 'CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c002e45e",
   "metadata": {
    "id": "9I8uiHsAESy7"
   },
   "source": [
    " The environment has an observation space of 4 continuous numbers:\n",
    "\n",
    "* Cart Position\n",
    "* Cart Velocity\n",
    "* Pole Angle\n",
    "* Pole Velocity At Tip\n",
    "\n",
    "To achieve this goal, the agent can take the following actions:\n",
    "\n",
    "* Push cart to the left\n",
    "* Push cart to the right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a4e58",
   "metadata": {
    "id": "YzCMPGXmFmvy"
   },
   "source": [
    "# **Interacting with the environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0006f",
   "metadata": {
    "id": "hL77-vqgFrQx"
   },
   "source": [
    "To make things clearer, let us now see how the agent can interact with the environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396bbd6d",
   "metadata": {
    "id": "a1TzPnj3Eu1i"
   },
   "source": [
    "**Getting the observation**\n",
    "\n",
    "(Every episode of training starts with env.reset(), to reset the simulation to the starting state or states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96485d1",
   "metadata": {
    "id": "if-9-u39Eu9v"
   },
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "print(\"Observation:\", observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41968019",
   "metadata": {
    "id": "GqCkMVE-E2a5"
   },
   "source": [
    "**Choosing an action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd5f14",
   "metadata": {
    "id": "xE5sKXohE3iV"
   },
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "print(\"Action chosen:\",action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ec915",
   "metadata": {
    "id": "Qi_dW1M7E3re"
   },
   "source": [
    "**Performing an environment step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7f474",
   "metadata": {
    "id": "K9iI68gyE69y"
   },
   "outputs": [],
   "source": [
    "next_observation, reward, done, _ = env.step(action) \n",
    "print(\"Next observation\", next_observation)\n",
    "print(\"Reward obtained\", reward)\n",
    "print(\"Does the state-action pair lead to a terminal state?\", done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf71fc",
   "metadata": {
    "id": "M4_e4XUrGyGj"
   },
   "source": [
    "**A few functions for rendering an episode on Google Colab and plotting rewards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c58d741c",
   "metadata": {
    "id": "1P-g1WjUG1XN"
   },
   "outputs": [],
   "source": [
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment \n",
    "and displaying it.\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env\n",
    "\n",
    "def plot_rewards(rewards, q, title):\n",
    "\n",
    "    avg_rew = []\n",
    "    j = 0\n",
    "    \n",
    "    while(j < len(rewards) - q):\n",
    "      \n",
    "      x = rewards[j:j+q]\n",
    "      sum1 = np.sum(np.array(x)) /q\n",
    "      avg_rew.append(sum1)\n",
    "      j = j+1\n",
    "\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.plot(avg_rew)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479bdeb",
   "metadata": {
    "id": "FMzWrnx-E7q6"
   },
   "source": [
    "**Task 1: Putting it all together - Episodes with a random agent**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca073ced",
   "metadata": {
    "id": "lxMzQlghx4mY"
   },
   "source": [
    "**Part 1**: Your task will be to run an episode of simulation with an agent following a random policy. After every 10 steps of the episode, **you are to print the step reward r as well as the cumulative reward from the beginning of the episode**. Complete the code and run the function to render the episode. The function returns the cumulative reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "885ff05a",
   "metadata": {
    "id": "9Gu80n-rE-8C"
   },
   "outputs": [],
   "source": [
    "def simulate_episode(env, wrap = False, render = False, video = False, log = False): #if log==True, print the step reward and cumulative reward\n",
    "    \n",
    "    if(wrap):\n",
    "      env = wrap_env(env) #This is purely for rendering\n",
    "    \n",
    "    observation = _ #initial observation\n",
    "    cumulative_reward = _\n",
    "\n",
    "    while True:\n",
    "\n",
    "        #Rendering function  \n",
    "        if(render):\n",
    "          env.render()\n",
    "        \n",
    "        #take an action, take a step and continue the episode if the next state is not terminal. Print required data every 10 steps\n",
    "\n",
    "\n",
    "                \n",
    "    env.close()\n",
    "    if(video):\n",
    "      show_video()\n",
    "    return cumulative_reward  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b76297",
   "metadata": {
    "id": "NuW7-rLubiH-"
   },
   "outputs": [],
   "source": [
    "total_rew = simulate_episode(env, wrap = True, render = True, video = True, log = True)\n",
    "print(\"Cumulative reward on episode termination\", total_rew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65800ef9",
   "metadata": {
    "id": "kIm6SfOxIpvq"
   },
   "source": [
    "**Part 2**: Using the function you just wrote, run a loop to simulate 1000 episodes of training and plot the cumulative reward vs episodes graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034953f",
   "metadata": {
    "id": "HW4Csz73IvIb"
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "ep = 0\n",
    "while(ep<1000):\n",
    "   \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200a30b",
   "metadata": {
    "id": "IoFS6sSGLLMP"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plot_rewards(rewards, 1, \"Random Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd9caf5",
   "metadata": {
    "id": "CmTfec97NO5l"
   },
   "source": [
    "# **Deep RL Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2c529a",
   "metadata": {
    "id": "Of5IWSAoHOiQ"
   },
   "source": [
    "Now that we know how to import an environment and use it, let us see how a deep RL agent trained on this task performs.\n",
    "\n",
    "Specifically, we use a Deep Q Network (DQN) to train our CartPole agent to navigate towards its goal. Broadly speaking, every RL algorithm performs an agent in two stages performed repeatedly till the task is performed optimally:\n",
    "\n",
    "\n",
    "\n",
    "1.   Collect episodes of data by interacting with the environment.\n",
    "2.   Learn to perform the task better, using the collected data.\n",
    "\n",
    "\n",
    "\n",
    "We do not get into further specifics of training the agent yet. This will covered in detail in future tutorials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd5b35",
   "metadata": {
    "id": "LZQeH0ZcDG-6"
   },
   "source": [
    "**How well does the DQN agent perform on the same task?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3710275",
   "metadata": {
    "id": "S4_QoylQOoUG"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "LR = 5e-4             # learning rate \n",
    "UPDATE_EVERY = 10        # how often to update the network (When Q target is present)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63d421ca",
   "metadata": {
    "id": "BKRAM4xjS7X5"
   },
   "outputs": [],
   "source": [
    "### Q NETWORK ARCHITECTURE\n",
    "'''\n",
    "QNetwork:\n",
    "Input Layer - 4 nodes (State Shape) \\\n",
    "Hidden Layer 1 - 64 nodes \\\n",
    "Hidden Layer 2 - 64 nodes \\\n",
    "Output Layer - 2 nodes (Action Space) \\\n",
    "Optimizer - zero_grad()\n",
    "'''\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2aa47de",
   "metadata": {
    "id": "zSeYYo6OS79L"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    CASE 1 -\n",
    "    +Q => Q-Targets\n",
    "    +E => Experience Replay\n",
    "    +T => Truncation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "\n",
    "        # Agent Environment Interaction\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # If enough samples are available in memory, get random subset and learn\n",
    "        if len(self.memory) >= BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "        \"\"\" +Q TARGETS PRESENT \"\"\"\n",
    "        # Updating the Network every 'UPDATE_EVERY' steps taken       \n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "\n",
    "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        \n",
    "\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        #Gradient Clipping\n",
    "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
    "        # for param in self.qnetwork_local.parameters():\n",
    "        #     param.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba524820",
   "metadata": {
    "id": "4Ux80vI_S-pd"
   },
   "outputs": [],
   "source": [
    "# REPLAY BUFFER\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56cba3ef",
   "metadata": {
    "id": "5CXkVBiUTANW"
   },
   "outputs": [],
   "source": [
    "def train(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.05, eps_decay=0.995):\n",
    "\n",
    "    scores = []                 # list containing scores from each episode\n",
    "    rewards = []\n",
    "    scores_window_printing = deque(maxlen=10) # For printing in the graph\n",
    "    scores_window= deque(maxlen=100)  # last 100 scores for checking if the avg is more than 195\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        rewards.append(score) \n",
    "        if(len(rewards)%500==0):\n",
    "                 plt.figure(figsize=(10, 5))\n",
    "                 plot_rewards(rewards, 1, \"DQN\")\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores_window_printing.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")  \n",
    "        if i_episode % 10 == 0: \n",
    "            scores.append(np.mean(scores_window_printing))        \n",
    "        if i_episode % 100 == 0: \n",
    "           print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        # if np.mean(scores_window)>=195.0:\n",
    "        #   print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "        #   break\n",
    "    return rewards, [np.array(scores),i_episode-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a88a25",
   "metadata": {
    "id": "XeuW0gNHfRjK"
   },
   "source": [
    "**Training the DQN agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8cfaa",
   "metadata": {
    "id": "7sZGkmOtTDot"
   },
   "outputs": [],
   "source": [
    "# Playing the game\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(0)\n",
    "state_shape = env.observation_space.shape[0]\n",
    "action_shape = env.action_space.n\n",
    "\n",
    "# Trial run to check if algorithm runs and saves the data\n",
    "begin_time = datetime.datetime.now()\n",
    "agent = Agent(state_size=state_shape,action_size = action_shape,seed = 0)\n",
    "rewards,_ = train(n_episodes=200)\n",
    "time_taken = datetime.datetime.now() - begin_time\n",
    "print(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718d1bc1",
   "metadata": {
    "id": "9xETfrEbaj-E"
   },
   "source": [
    "**Plotting the training curve of the DQN agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68303c7",
   "metadata": {
    "id": "6RyhsLCHajLO"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plot_rewards(rewards, 1, \"DQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653872aa",
   "metadata": {
    "id": "3g_rtlYrTUW4"
   },
   "source": [
    "**Task 2: Rendering an episode with the trained DQN agent**\n",
    "\n",
    "Your task will be to run an episode of simulation with the DQN agent. After every 50 steps of the episode, **you are to print the step reward r as well as the cumulative reward from the beginning of the episode**. Complete the code and run the function to render the episode. The function returns the cumulative reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a8771a",
   "metadata": {
    "id": "cu5tl9eS0h8W"
   },
   "outputs": [],
   "source": [
    "print(agent.act(env.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d349374",
   "metadata": {
    "id": "fn0ipuujulIS"
   },
   "outputs": [],
   "source": [
    "def simulate_episode_dqn(env, wrap = False, render = False, video = False, log = False): #if log==True, print the step reward and cumulative reward\n",
    "    \n",
    "    if(wrap):\n",
    "      env = wrap_env(env) \n",
    "    \n",
    "    \n",
    "    while True:\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "                \n",
    "    env.close()\n",
    "    if(video):\n",
    "      show_video()\n",
    "    return #__#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4e87f",
   "metadata": {
    "id": "FXoRZSzvtBJG"
   },
   "outputs": [],
   "source": [
    "#Make environment\n",
    "total_rew = simulate_episode_dqn(env, wrap = True, render = True, video = True, log = True)\n",
    "print(\"Cumulative reward after episode termination\", total_rew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7839d97b",
   "metadata": {
    "id": "bSzLKukxwlAc"
   },
   "source": [
    "## ------------------ **THANK YOU** ------------------- "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
